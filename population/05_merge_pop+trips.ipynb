{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a06239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df32903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move working directory up to acces data with relative paths\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf30da9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tjark/Documents/Python/CairoPopulation.nosync/tfc-git'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9edb24",
   "metadata": {},
   "source": [
    "### Import all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d087d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 04_Activity_Chains\n",
    "profiles = pd.read_csv('data/interim/activitychains/base_profiles.csv')\n",
    "\n",
    "trips = pd.read_csv('data/interim/activitychains/trips.csv')\n",
    "\n",
    "one_trip_chains = pd.read_csv('data/interim/activitychains/one_trip_chains.csv')\n",
    "two_trip_chains = pd.read_csv('data/interim/activitychains/two_trip_chains.csv')\n",
    "three_trip_chains = pd.read_csv('data/interim/activitychains/three_trip_chains.csv')\n",
    "four_trip_chains = pd.read_csv('data/interim/activitychains/four_trip_chains.csv')\n",
    "five_trip_chains = pd.read_csv('data/interim/activitychains/five_trip_chains.csv')\n",
    "six_trip_chains = pd.read_csv('data/interim/activitychains/six_trip_chains.csv')\n",
    "\n",
    "# from 02_generate_population\n",
    "scaled_pop_loc = pd.read_csv('data/interim/scaled_pop_loc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3776007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 02_generate_population\n",
    "scaled_pop_loc = pd.read_csv('data/interim/scaled_pop_loc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f85891",
   "metadata": {},
   "source": [
    "## Attach profiles based on age and sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4107de",
   "metadata": {},
   "source": [
    "#### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eccccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pop_loc = scaled_pop_loc[['AGE','SEX','ATTSCH','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba9b858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>educ</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.331430700000002 29.845431899999987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.313874 29.8144589)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.3075189 29.860689800000003)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  educ                                       geometry\n",
       "0   83  female   0.0  POINT (31.331430700000002 29.845431899999987)\n",
       "1   32  female   0.0                   POINT (31.313874 29.8144589)\n",
       "2   34  female   0.0          POINT (31.3075189 29.860689800000003)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_pop_loc['SEX'] = scaled_pop_loc['SEX'].replace(1, 'male')\n",
    "scaled_pop_loc['SEX'] = scaled_pop_loc['SEX'].replace(2, 'female')\n",
    "scaled_pop_loc.rename(columns={'SEX': 'gender', 'ATTSCH': 'educ', 'AGE': 'age'}, inplace=True)\n",
    "\n",
    "scaled_pop_loc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88270ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 0.14% of population as stay at home profiles\n",
    "stay_home_pop = 0.14\n",
    "scaled_pop_loc_active = scaled_pop_loc.sample(n=(round(len(scaled_pop_loc)*(1-stay_home_pop))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a000ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pop_loc_active['age_det'] = scaled_pop_loc_active['age']\n",
    "conditions = [\n",
    "    scaled_pop_loc_active['age'] < 18,\n",
    "    (scaled_pop_loc_active['age'] >= 18) & (scaled_pop_loc_active['age'] <= 25),\n",
    "    (scaled_pop_loc_active['age'] >= 26) & (scaled_pop_loc_active['age'] <= 35),\n",
    "    (scaled_pop_loc_active['age'] >= 36) & (scaled_pop_loc_active['age'] <= 45),\n",
    "    (scaled_pop_loc_active['age'] >= 46) & (scaled_pop_loc_active['age'] <= 55),\n",
    "    (scaled_pop_loc_active['age'] >= 56) & (scaled_pop_loc_active['age'] <= 65),\n",
    "    (scaled_pop_loc_active['age'] >= 66) & (scaled_pop_loc_active['age'] <= 75),\n",
    "]\n",
    "choices = ['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '66-75']\n",
    "\n",
    "scaled_pop_loc_active['age'] = np.select(conditions, choices, default='>75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f74093e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>educ</th>\n",
       "      <th>geometry</th>\n",
       "      <th>age_det</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7947229</th>\n",
       "      <td>&lt;18</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (31.317190321649655 30.15125319860771)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12417960</th>\n",
       "      <td>66-75</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.2503757 30.102899400000002)</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16038424</th>\n",
       "      <td>26-35</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.1875347 30.0911132)</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16543005</th>\n",
       "      <td>&lt;18</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (30.937156700000003 29.925582199999997)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599527</th>\n",
       "      <td>46-55</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.307901 30.138758899999996)</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  gender  educ                                       geometry  \\\n",
       "7947229     <18    male   1.0   POINT (31.317190321649655 30.15125319860771)   \n",
       "12417960  66-75  female   0.0          POINT (31.2503757 30.102899400000002)   \n",
       "16038424  26-35    male   0.0                  POINT (31.1875347 30.0911132)   \n",
       "16543005    <18    male   1.0  POINT (30.937156700000003 29.925582199999997)   \n",
       "12599527  46-55  female   0.0           POINT (31.307901 30.138758899999996)   \n",
       "\n",
       "          age_det  \n",
       "7947229         7  \n",
       "12417960       66  \n",
       "16038424       29  \n",
       "16543005        6  \n",
       "12599527       54  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_pop_loc_active.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ac8d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    scaled_pop_loc_active['educ'] == 0,\n",
    "    (scaled_pop_loc_active['age_det'] <12) &  scaled_pop_loc_active['educ'] == 1,\n",
    "    (scaled_pop_loc_active['age_det'] <18) &  scaled_pop_loc_active['educ'] == 1\n",
    "]\n",
    "choices = ['work', 'primary', 'secondary']\n",
    "\n",
    "scaled_pop_loc_active['main_act'] = np.select(conditions, choices, default='uni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e727ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d6af08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4449be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'profile_id': ['1605', '1606', '1607', '1608', '1609', '1610','1611','1612'],\n",
    "    'age': ['<18', '<18','<18', '<18', '>75', '>75', '>75', '>75'],\n",
    "    'gender': ['male', 'male', 'female', 'female', 'female', 'female', 'male', 'male'],\n",
    "    'vehicle_owned': [0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    'trips_count': [2, 3, 2, 3, 2, 2, 2, 2],\n",
    "    'activities': [1, 2, 1, 2, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "added_profiles = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e49dc222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/smcvnv_s04v84f_dnqvd4xkw0000gn/T/ipykernel_9192/2171748251.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  profiles = profiles.append(added_profiles, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "profiles = profiles.append(added_profiles, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "746b863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>vehicle_owned</th>\n",
       "      <th>trips_count</th>\n",
       "      <th>activities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18-25</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56-65</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18-25</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  profile_id    age  gender  vehicle_owned  trips_count  activities\n",
       "0          0  18-25  female              0            0           0\n",
       "1          1  56-65    male              0            2           1\n",
       "2          2  18-25  female              0            3           2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21986e32",
   "metadata": {},
   "source": [
    "### Merge profiles with pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be05d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scaled_pop_loc_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fabd802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20da178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 of 356\n",
      "Chunk 2 of 356\n",
      "Chunk 3 of 356\n",
      "Chunk 4 of 356\n",
      "Chunk 5 of 356\n",
      "Chunk 6 of 356\n",
      "Chunk 7 of 356\n",
      "Chunk 8 of 356\n",
      "Chunk 9 of 356\n",
      "Chunk 10 of 356\n",
      "Chunk 11 of 356\n",
      "Chunk 12 of 356\n",
      "Chunk 13 of 356\n",
      "Chunk 14 of 356\n",
      "Chunk 15 of 356\n",
      "Chunk 16 of 356\n",
      "Chunk 17 of 356\n",
      "Chunk 18 of 356\n",
      "Chunk 19 of 356\n",
      "Chunk 20 of 356\n",
      "Chunk 21 of 356\n",
      "Chunk 22 of 356\n",
      "Chunk 23 of 356\n",
      "Chunk 24 of 356\n",
      "Chunk 25 of 356\n",
      "Chunk 26 of 356\n",
      "Chunk 27 of 356\n",
      "Chunk 28 of 356\n",
      "Chunk 29 of 356\n",
      "Chunk 30 of 356\n",
      "Chunk 31 of 356\n",
      "Chunk 32 of 356\n",
      "Chunk 33 of 356\n",
      "Chunk 34 of 356\n",
      "Chunk 35 of 356\n",
      "Chunk 36 of 356\n",
      "Chunk 37 of 356\n",
      "Chunk 38 of 356\n",
      "Chunk 39 of 356\n",
      "Chunk 40 of 356\n",
      "Chunk 41 of 356\n",
      "Chunk 42 of 356\n",
      "Chunk 43 of 356\n",
      "Chunk 44 of 356\n",
      "Chunk 45 of 356\n",
      "Chunk 46 of 356\n",
      "Chunk 47 of 356\n",
      "Chunk 48 of 356\n",
      "Chunk 49 of 356\n",
      "Chunk 50 of 356\n",
      "Chunk 51 of 356\n",
      "Chunk 52 of 356\n",
      "Chunk 53 of 356\n",
      "Chunk 54 of 356\n",
      "Chunk 55 of 356\n",
      "Chunk 56 of 356\n",
      "Chunk 57 of 356\n",
      "Chunk 58 of 356\n",
      "Chunk 59 of 356\n",
      "Chunk 60 of 356\n",
      "Chunk 61 of 356\n",
      "Chunk 62 of 356\n",
      "Chunk 63 of 356\n",
      "Chunk 64 of 356\n",
      "Chunk 65 of 356\n",
      "Chunk 66 of 356\n",
      "Chunk 67 of 356\n",
      "Chunk 68 of 356\n",
      "Chunk 69 of 356\n",
      "Chunk 70 of 356\n",
      "Chunk 71 of 356\n",
      "Chunk 72 of 356\n",
      "Chunk 73 of 356\n",
      "Chunk 74 of 356\n",
      "Chunk 75 of 356\n",
      "Chunk 76 of 356\n",
      "Chunk 77 of 356\n",
      "Chunk 78 of 356\n",
      "Chunk 79 of 356\n",
      "Chunk 80 of 356\n",
      "Chunk 81 of 356\n",
      "Chunk 82 of 356\n",
      "Chunk 83 of 356\n",
      "Chunk 84 of 356\n",
      "Chunk 85 of 356\n",
      "Chunk 86 of 356\n",
      "Chunk 87 of 356\n",
      "Chunk 88 of 356\n",
      "Chunk 89 of 356\n",
      "Chunk 90 of 356\n",
      "Chunk 91 of 356\n",
      "Chunk 92 of 356\n",
      "Chunk 93 of 356\n",
      "Chunk 94 of 356\n",
      "Chunk 95 of 356\n",
      "Chunk 96 of 356\n",
      "Chunk 97 of 356\n",
      "Chunk 98 of 356\n",
      "Chunk 99 of 356\n",
      "Chunk 100 of 356\n",
      "Chunk 101 of 356\n",
      "Chunk 102 of 356\n",
      "Chunk 103 of 356\n",
      "Chunk 104 of 356\n",
      "Chunk 105 of 356\n",
      "Chunk 106 of 356\n",
      "Chunk 107 of 356\n",
      "Chunk 108 of 356\n",
      "Chunk 109 of 356\n",
      "Chunk 110 of 356\n",
      "Chunk 111 of 356\n",
      "Chunk 112 of 356\n",
      "Chunk 113 of 356\n",
      "Chunk 114 of 356\n",
      "Chunk 115 of 356\n",
      "Chunk 116 of 356\n",
      "Chunk 117 of 356\n",
      "Chunk 118 of 356\n",
      "Chunk 119 of 356\n",
      "Chunk 120 of 356\n",
      "Chunk 121 of 356\n",
      "Chunk 122 of 356\n",
      "Chunk 123 of 356\n",
      "Chunk 124 of 356\n",
      "Chunk 125 of 356\n",
      "Chunk 126 of 356\n",
      "Chunk 127 of 356\n",
      "Chunk 128 of 356\n",
      "Chunk 129 of 356\n",
      "Chunk 130 of 356\n",
      "Chunk 131 of 356\n",
      "Chunk 132 of 356\n",
      "Chunk 133 of 356\n",
      "Chunk 134 of 356\n",
      "Chunk 135 of 356\n",
      "Chunk 136 of 356\n",
      "Chunk 137 of 356\n",
      "Chunk 138 of 356\n",
      "Chunk 139 of 356\n",
      "Chunk 140 of 356\n",
      "Chunk 141 of 356\n",
      "Chunk 142 of 356\n",
      "Chunk 143 of 356\n",
      "Chunk 144 of 356\n",
      "Chunk 145 of 356\n",
      "Chunk 146 of 356\n",
      "Chunk 147 of 356\n",
      "Chunk 148 of 356\n",
      "Chunk 149 of 356\n",
      "Chunk 150 of 356\n",
      "Chunk 151 of 356\n",
      "Chunk 152 of 356\n",
      "Chunk 153 of 356\n",
      "Chunk 154 of 356\n",
      "Chunk 155 of 356\n",
      "Chunk 156 of 356\n",
      "Chunk 157 of 356\n",
      "Chunk 158 of 356\n",
      "Chunk 159 of 356\n",
      "Chunk 160 of 356\n",
      "Chunk 161 of 356\n",
      "Chunk 162 of 356\n",
      "Chunk 163 of 356\n",
      "Chunk 164 of 356\n",
      "Chunk 165 of 356\n",
      "Chunk 166 of 356\n",
      "Chunk 167 of 356\n",
      "Chunk 168 of 356\n",
      "Chunk 169 of 356\n",
      "Chunk 170 of 356\n",
      "Chunk 171 of 356\n",
      "Chunk 172 of 356\n",
      "Chunk 173 of 356\n",
      "Chunk 174 of 356\n",
      "Chunk 175 of 356\n",
      "Chunk 176 of 356\n",
      "Chunk 177 of 356\n",
      "Chunk 178 of 356\n",
      "Chunk 179 of 356\n",
      "Chunk 180 of 356\n",
      "Chunk 181 of 356\n",
      "Chunk 182 of 356\n",
      "Chunk 183 of 356\n",
      "Chunk 184 of 356\n",
      "Chunk 185 of 356\n",
      "Chunk 186 of 356\n",
      "Chunk 187 of 356\n",
      "Chunk 188 of 356\n",
      "Chunk 189 of 356\n",
      "Chunk 190 of 356\n",
      "Chunk 191 of 356\n",
      "Chunk 192 of 356\n",
      "Chunk 193 of 356\n",
      "Chunk 194 of 356\n",
      "Chunk 195 of 356\n",
      "Chunk 196 of 356\n",
      "Chunk 197 of 356\n",
      "Chunk 198 of 356\n",
      "Chunk 199 of 356\n",
      "Chunk 200 of 356\n",
      "Chunk 201 of 356\n",
      "Chunk 202 of 356\n",
      "Chunk 203 of 356\n",
      "Chunk 204 of 356\n",
      "Chunk 205 of 356\n",
      "Chunk 206 of 356\n",
      "Chunk 207 of 356\n",
      "Chunk 208 of 356\n",
      "Chunk 209 of 356\n",
      "Chunk 210 of 356\n",
      "Chunk 211 of 356\n",
      "Chunk 212 of 356\n",
      "Chunk 213 of 356\n",
      "Chunk 214 of 356\n",
      "Chunk 215 of 356\n",
      "Chunk 216 of 356\n",
      "Chunk 217 of 356\n",
      "Chunk 218 of 356\n",
      "Chunk 219 of 356\n",
      "Chunk 220 of 356\n",
      "Chunk 221 of 356\n",
      "Chunk 222 of 356\n",
      "Chunk 223 of 356\n",
      "Chunk 224 of 356\n",
      "Chunk 225 of 356\n",
      "Chunk 226 of 356\n",
      "Chunk 227 of 356\n",
      "Chunk 228 of 356\n",
      "Chunk 229 of 356\n",
      "Chunk 230 of 356\n",
      "Chunk 231 of 356\n",
      "Chunk 232 of 356\n",
      "Chunk 233 of 356\n",
      "Chunk 234 of 356\n",
      "Chunk 235 of 356\n",
      "Chunk 236 of 356\n",
      "Chunk 237 of 356\n",
      "Chunk 238 of 356\n",
      "Chunk 239 of 356\n",
      "Chunk 240 of 356\n",
      "Chunk 241 of 356\n",
      "Chunk 242 of 356\n",
      "Chunk 243 of 356\n",
      "Chunk 244 of 356\n",
      "Chunk 245 of 356\n",
      "Chunk 246 of 356\n",
      "Chunk 247 of 356\n",
      "Chunk 248 of 356\n",
      "Chunk 249 of 356\n",
      "Chunk 250 of 356\n",
      "Chunk 251 of 356\n",
      "Chunk 252 of 356\n",
      "Chunk 253 of 356\n",
      "Chunk 254 of 356\n",
      "Chunk 255 of 356\n",
      "Chunk 256 of 356\n",
      "Chunk 257 of 356\n",
      "Chunk 258 of 356\n",
      "Chunk 259 of 356\n",
      "Chunk 260 of 356\n",
      "Chunk 261 of 356\n",
      "Chunk 262 of 356\n",
      "Chunk 263 of 356\n",
      "Chunk 264 of 356\n",
      "Chunk 265 of 356\n",
      "Chunk 266 of 356\n",
      "Chunk 267 of 356\n",
      "Chunk 268 of 356\n",
      "Chunk 269 of 356\n",
      "Chunk 270 of 356\n",
      "Chunk 271 of 356\n",
      "Chunk 272 of 356\n",
      "Chunk 273 of 356\n",
      "Chunk 274 of 356\n",
      "Chunk 275 of 356\n",
      "Chunk 276 of 356\n",
      "Chunk 277 of 356\n",
      "Chunk 278 of 356\n",
      "Chunk 279 of 356\n",
      "Chunk 280 of 356\n",
      "Chunk 281 of 356\n",
      "Chunk 282 of 356\n",
      "Chunk 283 of 356\n",
      "Chunk 284 of 356\n",
      "Chunk 285 of 356\n",
      "Chunk 286 of 356\n",
      "Chunk 287 of 356\n",
      "Chunk 288 of 356\n",
      "Chunk 289 of 356\n",
      "Chunk 290 of 356\n",
      "Chunk 291 of 356\n",
      "Chunk 292 of 356\n",
      "Chunk 293 of 356\n",
      "Chunk 294 of 356\n",
      "Chunk 295 of 356\n",
      "Chunk 296 of 356\n",
      "Chunk 297 of 356\n",
      "Chunk 298 of 356\n",
      "Chunk 299 of 356\n",
      "Chunk 300 of 356\n",
      "Chunk 301 of 356\n",
      "Chunk 302 of 356\n",
      "Chunk 303 of 356\n",
      "Chunk 304 of 356\n",
      "Chunk 305 of 356\n",
      "Chunk 306 of 356\n",
      "Chunk 307 of 356\n",
      "Chunk 308 of 356\n",
      "Chunk 309 of 356\n",
      "Chunk 310 of 356\n",
      "Chunk 311 of 356\n",
      "Chunk 312 of 356\n",
      "Chunk 313 of 356\n",
      "Chunk 314 of 356\n",
      "Chunk 315 of 356\n",
      "Chunk 316 of 356\n",
      "Chunk 317 of 356\n",
      "Chunk 318 of 356\n",
      "Chunk 319 of 356\n",
      "Chunk 320 of 356\n",
      "Chunk 321 of 356\n",
      "Chunk 322 of 356\n",
      "Chunk 323 of 356\n",
      "Chunk 324 of 356\n",
      "Chunk 325 of 356\n",
      "Chunk 326 of 356\n",
      "Chunk 327 of 356\n",
      "Chunk 328 of 356\n",
      "Chunk 329 of 356\n",
      "Chunk 330 of 356\n",
      "Chunk 331 of 356\n",
      "Chunk 332 of 356\n",
      "Chunk 333 of 356\n",
      "Chunk 334 of 356\n",
      "Chunk 335 of 356\n",
      "Chunk 336 of 356\n",
      "Chunk 337 of 356\n",
      "Chunk 338 of 356\n",
      "Chunk 339 of 356\n",
      "Chunk 340 of 356\n",
      "Chunk 341 of 356\n",
      "Chunk 342 of 356\n",
      "Chunk 343 of 356\n",
      "Chunk 344 of 356\n",
      "Chunk 345 of 356\n",
      "Chunk 346 of 356\n",
      "Chunk 347 of 356\n",
      "Chunk 348 of 356\n",
      "Chunk 349 of 356\n",
      "Chunk 350 of 356\n",
      "Chunk 351 of 356\n",
      "Chunk 352 of 356\n",
      "Chunk 353 of 356\n",
      "Chunk 354 of 356\n",
      "Chunk 355 of 356\n",
      "Chunk 356 of 356\n"
     ]
    }
   ],
   "source": [
    "# Define the chunk size\n",
    "chunksize = 50000\n",
    "\n",
    "# Create an empty list to store the merged chunks\n",
    "merged_chunks = []\n",
    "\n",
    "# Get the total number of chunks\n",
    "total_chunks = -(-len(df) // chunksize)  # Round up division\n",
    "\n",
    "# Iterate over the data in chunks\n",
    "for i in range(total_chunks):\n",
    "    start = i * chunksize\n",
    "    end = (i + 1) * chunksize\n",
    "    # Get the chunk of data\n",
    "    chunk_df = df[start:end]\n",
    "    # Perform the merge operation on each chunk\n",
    "    merged_chunk = pd.merge(chunk_df, profiles, on=['age', 'gender'], how='left')    \n",
    "    # Append the merged chunk to the list\n",
    "    merged_chunks.append(merged_chunk)\n",
    "    print(f'Chunk {i+1} of {total_chunks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf2924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped and sampled for chunk 1 of 356.\n",
      "Grouped and sampled for chunk 2 of 356.\n",
      "Grouped and sampled for chunk 3 of 356.\n",
      "Grouped and sampled for chunk 4 of 356.\n",
      "Grouped and sampled for chunk 5 of 356.\n",
      "Grouped and sampled for chunk 6 of 356.\n",
      "Grouped and sampled for chunk 7 of 356.\n",
      "Grouped and sampled for chunk 8 of 356.\n",
      "Grouped and sampled for chunk 9 of 356.\n",
      "Grouped and sampled for chunk 10 of 356.\n",
      "Grouped and sampled for chunk 11 of 356.\n",
      "Grouped and sampled for chunk 12 of 356.\n",
      "Grouped and sampled for chunk 13 of 356.\n",
      "Grouped and sampled for chunk 14 of 356.\n",
      "Grouped and sampled for chunk 15 of 356.\n",
      "Grouped and sampled for chunk 16 of 356.\n",
      "Grouped and sampled for chunk 17 of 356.\n",
      "Grouped and sampled for chunk 18 of 356.\n",
      "Grouped and sampled for chunk 19 of 356.\n",
      "Grouped and sampled for chunk 20 of 356.\n",
      "Grouped and sampled for chunk 21 of 356.\n",
      "Grouped and sampled for chunk 22 of 356.\n",
      "Grouped and sampled for chunk 23 of 356.\n",
      "Grouped and sampled for chunk 24 of 356.\n",
      "Grouped and sampled for chunk 25 of 356.\n",
      "Grouped and sampled for chunk 26 of 356.\n",
      "Grouped and sampled for chunk 27 of 356.\n",
      "Grouped and sampled for chunk 28 of 356.\n",
      "Grouped and sampled for chunk 29 of 356.\n",
      "Grouped and sampled for chunk 30 of 356.\n",
      "Grouped and sampled for chunk 31 of 356.\n",
      "Grouped and sampled for chunk 32 of 356.\n",
      "Grouped and sampled for chunk 33 of 356.\n",
      "Grouped and sampled for chunk 34 of 356.\n",
      "Grouped and sampled for chunk 35 of 356.\n",
      "Grouped and sampled for chunk 36 of 356.\n",
      "Grouped and sampled for chunk 37 of 356.\n",
      "Grouped and sampled for chunk 38 of 356.\n",
      "Grouped and sampled for chunk 39 of 356.\n",
      "Grouped and sampled for chunk 40 of 356.\n",
      "Grouped and sampled for chunk 41 of 356.\n",
      "Grouped and sampled for chunk 42 of 356.\n",
      "Grouped and sampled for chunk 43 of 356.\n",
      "Grouped and sampled for chunk 44 of 356.\n",
      "Grouped and sampled for chunk 45 of 356.\n",
      "Grouped and sampled for chunk 46 of 356.\n",
      "Grouped and sampled for chunk 47 of 356.\n",
      "Grouped and sampled for chunk 48 of 356.\n",
      "Grouped and sampled for chunk 49 of 356.\n",
      "Grouped and sampled for chunk 50 of 356.\n",
      "Grouped and sampled for chunk 51 of 356.\n",
      "Grouped and sampled for chunk 52 of 356.\n",
      "Grouped and sampled for chunk 53 of 356.\n",
      "Grouped and sampled for chunk 54 of 356.\n",
      "Grouped and sampled for chunk 55 of 356.\n",
      "Grouped and sampled for chunk 56 of 356.\n",
      "Grouped and sampled for chunk 57 of 356.\n",
      "Grouped and sampled for chunk 58 of 356.\n",
      "Grouped and sampled for chunk 59 of 356.\n",
      "Grouped and sampled for chunk 60 of 356.\n",
      "Grouped and sampled for chunk 61 of 356.\n",
      "Grouped and sampled for chunk 62 of 356.\n",
      "Grouped and sampled for chunk 63 of 356.\n",
      "Grouped and sampled for chunk 64 of 356.\n",
      "Grouped and sampled for chunk 65 of 356.\n",
      "Grouped and sampled for chunk 66 of 356.\n",
      "Grouped and sampled for chunk 67 of 356.\n",
      "Grouped and sampled for chunk 68 of 356.\n",
      "Grouped and sampled for chunk 69 of 356.\n",
      "Grouped and sampled for chunk 70 of 356.\n",
      "Grouped and sampled for chunk 71 of 356.\n",
      "Grouped and sampled for chunk 72 of 356.\n",
      "Grouped and sampled for chunk 73 of 356.\n",
      "Grouped and sampled for chunk 74 of 356.\n",
      "Grouped and sampled for chunk 75 of 356.\n",
      "Grouped and sampled for chunk 76 of 356.\n",
      "Grouped and sampled for chunk 77 of 356.\n",
      "Grouped and sampled for chunk 78 of 356.\n",
      "Grouped and sampled for chunk 79 of 356.\n",
      "Grouped and sampled for chunk 80 of 356.\n",
      "Grouped and sampled for chunk 81 of 356.\n",
      "Grouped and sampled for chunk 82 of 356.\n",
      "Grouped and sampled for chunk 83 of 356.\n",
      "Grouped and sampled for chunk 84 of 356.\n",
      "Grouped and sampled for chunk 85 of 356.\n",
      "Grouped and sampled for chunk 86 of 356.\n",
      "Grouped and sampled for chunk 87 of 356.\n",
      "Grouped and sampled for chunk 88 of 356.\n",
      "Grouped and sampled for chunk 89 of 356.\n",
      "Grouped and sampled for chunk 90 of 356.\n",
      "Grouped and sampled for chunk 91 of 356.\n",
      "Grouped and sampled for chunk 92 of 356.\n",
      "Grouped and sampled for chunk 93 of 356.\n",
      "Grouped and sampled for chunk 94 of 356.\n",
      "Grouped and sampled for chunk 95 of 356.\n",
      "Grouped and sampled for chunk 96 of 356.\n",
      "Grouped and sampled for chunk 97 of 356.\n",
      "Grouped and sampled for chunk 98 of 356.\n",
      "Grouped and sampled for chunk 99 of 356.\n",
      "Grouped and sampled for chunk 100 of 356.\n",
      "Grouped and sampled for chunk 101 of 356.\n",
      "Grouped and sampled for chunk 102 of 356.\n",
      "Grouped and sampled for chunk 103 of 356.\n",
      "Grouped and sampled for chunk 104 of 356.\n",
      "Grouped and sampled for chunk 105 of 356.\n",
      "Grouped and sampled for chunk 106 of 356.\n",
      "Grouped and sampled for chunk 107 of 356.\n",
      "Grouped and sampled for chunk 108 of 356.\n",
      "Grouped and sampled for chunk 109 of 356.\n",
      "Grouped and sampled for chunk 110 of 356.\n",
      "Grouped and sampled for chunk 111 of 356.\n",
      "Grouped and sampled for chunk 112 of 356.\n",
      "Grouped and sampled for chunk 113 of 356.\n",
      "Grouped and sampled for chunk 114 of 356.\n",
      "Grouped and sampled for chunk 115 of 356.\n",
      "Grouped and sampled for chunk 116 of 356.\n",
      "Grouped and sampled for chunk 117 of 356.\n",
      "Grouped and sampled for chunk 118 of 356.\n",
      "Grouped and sampled for chunk 119 of 356.\n",
      "Grouped and sampled for chunk 120 of 356.\n",
      "Grouped and sampled for chunk 121 of 356.\n",
      "Grouped and sampled for chunk 122 of 356.\n",
      "Grouped and sampled for chunk 123 of 356.\n",
      "Grouped and sampled for chunk 124 of 356.\n",
      "Grouped and sampled for chunk 125 of 356.\n",
      "Grouped and sampled for chunk 126 of 356.\n",
      "Grouped and sampled for chunk 127 of 356.\n",
      "Grouped and sampled for chunk 128 of 356.\n",
      "Grouped and sampled for chunk 129 of 356.\n",
      "Grouped and sampled for chunk 130 of 356.\n",
      "Grouped and sampled for chunk 131 of 356.\n",
      "Grouped and sampled for chunk 132 of 356.\n",
      "Grouped and sampled for chunk 133 of 356.\n",
      "Grouped and sampled for chunk 134 of 356.\n",
      "Grouped and sampled for chunk 135 of 356.\n",
      "Grouped and sampled for chunk 136 of 356.\n",
      "Grouped and sampled for chunk 137 of 356.\n",
      "Grouped and sampled for chunk 138 of 356.\n",
      "Grouped and sampled for chunk 139 of 356.\n",
      "Grouped and sampled for chunk 140 of 356.\n",
      "Grouped and sampled for chunk 141 of 356.\n",
      "Grouped and sampled for chunk 142 of 356.\n",
      "Grouped and sampled for chunk 143 of 356.\n",
      "Grouped and sampled for chunk 144 of 356.\n",
      "Grouped and sampled for chunk 145 of 356.\n",
      "Grouped and sampled for chunk 146 of 356.\n",
      "Grouped and sampled for chunk 147 of 356.\n",
      "Grouped and sampled for chunk 148 of 356.\n",
      "Grouped and sampled for chunk 149 of 356.\n",
      "Grouped and sampled for chunk 150 of 356.\n",
      "Grouped and sampled for chunk 151 of 356.\n",
      "Grouped and sampled for chunk 152 of 356.\n",
      "Grouped and sampled for chunk 153 of 356.\n",
      "Grouped and sampled for chunk 154 of 356.\n",
      "Grouped and sampled for chunk 155 of 356.\n",
      "Grouped and sampled for chunk 156 of 356.\n",
      "Grouped and sampled for chunk 157 of 356.\n",
      "Grouped and sampled for chunk 158 of 356.\n",
      "Grouped and sampled for chunk 159 of 356.\n",
      "Grouped and sampled for chunk 160 of 356.\n",
      "Grouped and sampled for chunk 161 of 356.\n",
      "Grouped and sampled for chunk 162 of 356.\n",
      "Grouped and sampled for chunk 163 of 356.\n",
      "Grouped and sampled for chunk 164 of 356.\n",
      "Grouped and sampled for chunk 165 of 356.\n",
      "Grouped and sampled for chunk 166 of 356.\n",
      "Grouped and sampled for chunk 167 of 356.\n",
      "Grouped and sampled for chunk 168 of 356.\n",
      "Grouped and sampled for chunk 169 of 356.\n",
      "Grouped and sampled for chunk 170 of 356.\n",
      "Grouped and sampled for chunk 171 of 356.\n",
      "Grouped and sampled for chunk 172 of 356.\n",
      "Grouped and sampled for chunk 173 of 356.\n",
      "Grouped and sampled for chunk 174 of 356.\n",
      "Grouped and sampled for chunk 175 of 356.\n",
      "Grouped and sampled for chunk 176 of 356.\n",
      "Grouped and sampled for chunk 177 of 356.\n",
      "Grouped and sampled for chunk 178 of 356.\n",
      "Grouped and sampled for chunk 179 of 356.\n",
      "Grouped and sampled for chunk 180 of 356.\n",
      "Grouped and sampled for chunk 181 of 356.\n",
      "Grouped and sampled for chunk 182 of 356.\n",
      "Grouped and sampled for chunk 183 of 356.\n",
      "Grouped and sampled for chunk 184 of 356.\n",
      "Grouped and sampled for chunk 185 of 356.\n",
      "Grouped and sampled for chunk 186 of 356.\n",
      "Grouped and sampled for chunk 187 of 356.\n",
      "Grouped and sampled for chunk 188 of 356.\n",
      "Grouped and sampled for chunk 189 of 356.\n",
      "Grouped and sampled for chunk 190 of 356.\n",
      "Grouped and sampled for chunk 191 of 356.\n",
      "Grouped and sampled for chunk 192 of 356.\n",
      "Grouped and sampled for chunk 193 of 356.\n",
      "Grouped and sampled for chunk 194 of 356.\n",
      "Grouped and sampled for chunk 195 of 356.\n",
      "Grouped and sampled for chunk 196 of 356.\n",
      "Grouped and sampled for chunk 197 of 356.\n",
      "Grouped and sampled for chunk 198 of 356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped and sampled for chunk 199 of 356.\n",
      "Grouped and sampled for chunk 200 of 356.\n",
      "Grouped and sampled for chunk 201 of 356.\n",
      "Grouped and sampled for chunk 202 of 356.\n",
      "Grouped and sampled for chunk 203 of 356.\n",
      "Grouped and sampled for chunk 204 of 356.\n",
      "Grouped and sampled for chunk 205 of 356.\n",
      "Grouped and sampled for chunk 206 of 356.\n",
      "Grouped and sampled for chunk 207 of 356.\n",
      "Grouped and sampled for chunk 208 of 356.\n",
      "Grouped and sampled for chunk 209 of 356.\n",
      "Grouped and sampled for chunk 210 of 356.\n",
      "Grouped and sampled for chunk 211 of 356.\n",
      "Grouped and sampled for chunk 212 of 356.\n",
      "Grouped and sampled for chunk 213 of 356.\n",
      "Grouped and sampled for chunk 214 of 356.\n",
      "Grouped and sampled for chunk 215 of 356.\n",
      "Grouped and sampled for chunk 216 of 356.\n",
      "Grouped and sampled for chunk 217 of 356.\n",
      "Grouped and sampled for chunk 218 of 356.\n",
      "Grouped and sampled for chunk 219 of 356.\n",
      "Grouped and sampled for chunk 220 of 356.\n",
      "Grouped and sampled for chunk 221 of 356.\n",
      "Grouped and sampled for chunk 222 of 356.\n",
      "Grouped and sampled for chunk 223 of 356.\n",
      "Grouped and sampled for chunk 224 of 356.\n",
      "Grouped and sampled for chunk 225 of 356.\n",
      "Grouped and sampled for chunk 226 of 356.\n",
      "Grouped and sampled for chunk 227 of 356.\n",
      "Grouped and sampled for chunk 228 of 356.\n",
      "Grouped and sampled for chunk 229 of 356.\n",
      "Grouped and sampled for chunk 230 of 356.\n",
      "Grouped and sampled for chunk 231 of 356.\n",
      "Grouped and sampled for chunk 232 of 356.\n",
      "Grouped and sampled for chunk 233 of 356.\n",
      "Grouped and sampled for chunk 234 of 356.\n",
      "Grouped and sampled for chunk 235 of 356.\n",
      "Grouped and sampled for chunk 236 of 356.\n",
      "Grouped and sampled for chunk 237 of 356.\n",
      "Grouped and sampled for chunk 238 of 356.\n",
      "Grouped and sampled for chunk 239 of 356.\n",
      "Grouped and sampled for chunk 240 of 356.\n",
      "Grouped and sampled for chunk 241 of 356.\n",
      "Grouped and sampled for chunk 242 of 356.\n",
      "Grouped and sampled for chunk 243 of 356.\n",
      "Grouped and sampled for chunk 244 of 356.\n",
      "Grouped and sampled for chunk 245 of 356.\n",
      "Grouped and sampled for chunk 246 of 356.\n",
      "Grouped and sampled for chunk 247 of 356.\n",
      "Grouped and sampled for chunk 248 of 356.\n",
      "Grouped and sampled for chunk 249 of 356.\n",
      "Grouped and sampled for chunk 250 of 356.\n",
      "Grouped and sampled for chunk 251 of 356.\n",
      "Grouped and sampled for chunk 252 of 356.\n",
      "Grouped and sampled for chunk 253 of 356.\n",
      "Grouped and sampled for chunk 254 of 356.\n",
      "Grouped and sampled for chunk 255 of 356.\n",
      "Grouped and sampled for chunk 256 of 356.\n",
      "Grouped and sampled for chunk 257 of 356.\n",
      "Grouped and sampled for chunk 258 of 356.\n",
      "Grouped and sampled for chunk 259 of 356.\n",
      "Grouped and sampled for chunk 260 of 356.\n",
      "Grouped and sampled for chunk 261 of 356.\n",
      "Grouped and sampled for chunk 262 of 356.\n",
      "Grouped and sampled for chunk 263 of 356.\n",
      "Grouped and sampled for chunk 264 of 356.\n",
      "Grouped and sampled for chunk 265 of 356.\n",
      "Grouped and sampled for chunk 266 of 356.\n",
      "Grouped and sampled for chunk 267 of 356.\n",
      "Grouped and sampled for chunk 268 of 356.\n",
      "Grouped and sampled for chunk 269 of 356.\n",
      "Grouped and sampled for chunk 270 of 356.\n",
      "Grouped and sampled for chunk 271 of 356.\n",
      "Grouped and sampled for chunk 272 of 356.\n",
      "Grouped and sampled for chunk 273 of 356.\n",
      "Grouped and sampled for chunk 274 of 356.\n",
      "Grouped and sampled for chunk 275 of 356.\n",
      "Grouped and sampled for chunk 276 of 356.\n",
      "Grouped and sampled for chunk 277 of 356.\n",
      "Grouped and sampled for chunk 278 of 356.\n",
      "Grouped and sampled for chunk 279 of 356.\n",
      "Grouped and sampled for chunk 280 of 356.\n",
      "Grouped and sampled for chunk 281 of 356.\n",
      "Grouped and sampled for chunk 282 of 356.\n",
      "Grouped and sampled for chunk 283 of 356.\n",
      "Grouped and sampled for chunk 284 of 356.\n",
      "Grouped and sampled for chunk 285 of 356.\n",
      "Grouped and sampled for chunk 286 of 356.\n",
      "Grouped and sampled for chunk 287 of 356.\n",
      "Grouped and sampled for chunk 288 of 356.\n",
      "Grouped and sampled for chunk 289 of 356.\n",
      "Grouped and sampled for chunk 290 of 356.\n",
      "Grouped and sampled for chunk 291 of 356.\n",
      "Grouped and sampled for chunk 292 of 356.\n",
      "Grouped and sampled for chunk 293 of 356.\n",
      "Grouped and sampled for chunk 294 of 356.\n",
      "Grouped and sampled for chunk 295 of 356.\n",
      "Grouped and sampled for chunk 296 of 356.\n",
      "Grouped and sampled for chunk 297 of 356.\n",
      "Grouped and sampled for chunk 298 of 356.\n",
      "Grouped and sampled for chunk 299 of 356.\n",
      "Grouped and sampled for chunk 300 of 356.\n",
      "Grouped and sampled for chunk 301 of 356.\n",
      "Grouped and sampled for chunk 302 of 356.\n",
      "Grouped and sampled for chunk 303 of 356.\n",
      "Grouped and sampled for chunk 304 of 356.\n",
      "Grouped and sampled for chunk 305 of 356.\n",
      "Grouped and sampled for chunk 306 of 356.\n",
      "Grouped and sampled for chunk 307 of 356.\n",
      "Grouped and sampled for chunk 308 of 356.\n",
      "Grouped and sampled for chunk 309 of 356.\n",
      "Grouped and sampled for chunk 310 of 356.\n",
      "Grouped and sampled for chunk 311 of 356.\n",
      "Grouped and sampled for chunk 312 of 356.\n",
      "Grouped and sampled for chunk 313 of 356.\n",
      "Grouped and sampled for chunk 314 of 356.\n",
      "Grouped and sampled for chunk 315 of 356.\n",
      "Grouped and sampled for chunk 316 of 356.\n",
      "Grouped and sampled for chunk 317 of 356.\n",
      "Grouped and sampled for chunk 318 of 356.\n",
      "Grouped and sampled for chunk 319 of 356.\n",
      "Grouped and sampled for chunk 320 of 356.\n",
      "Grouped and sampled for chunk 321 of 356.\n",
      "Grouped and sampled for chunk 322 of 356.\n",
      "Grouped and sampled for chunk 323 of 356.\n",
      "Grouped and sampled for chunk 324 of 356.\n",
      "Grouped and sampled for chunk 325 of 356.\n",
      "Grouped and sampled for chunk 326 of 356.\n",
      "Grouped and sampled for chunk 327 of 356.\n",
      "Grouped and sampled for chunk 328 of 356.\n",
      "Grouped and sampled for chunk 329 of 356.\n",
      "Grouped and sampled for chunk 330 of 356.\n",
      "Grouped and sampled for chunk 331 of 356.\n",
      "Grouped and sampled for chunk 332 of 356.\n",
      "Grouped and sampled for chunk 333 of 356.\n",
      "Grouped and sampled for chunk 334 of 356.\n",
      "Grouped and sampled for chunk 335 of 356.\n",
      "Grouped and sampled for chunk 336 of 356.\n",
      "Grouped and sampled for chunk 337 of 356.\n",
      "Grouped and sampled for chunk 338 of 356.\n",
      "Grouped and sampled for chunk 339 of 356.\n",
      "Grouped and sampled for chunk 340 of 356.\n",
      "Grouped and sampled for chunk 341 of 356.\n",
      "Grouped and sampled for chunk 342 of 356.\n",
      "Grouped and sampled for chunk 343 of 356.\n",
      "Grouped and sampled for chunk 344 of 356.\n",
      "Grouped and sampled for chunk 345 of 356.\n",
      "Grouped and sampled for chunk 346 of 356.\n",
      "Grouped and sampled for chunk 347 of 356.\n",
      "Grouped and sampled for chunk 348 of 356.\n",
      "Grouped and sampled for chunk 349 of 356.\n",
      "Grouped and sampled for chunk 350 of 356.\n",
      "Grouped and sampled for chunk 351 of 356.\n",
      "Grouped and sampled for chunk 352 of 356.\n",
      "Grouped and sampled for chunk 353 of 356.\n",
      "Grouped and sampled for chunk 354 of 356.\n",
      "Grouped and sampled for chunk 355 of 356.\n",
      "Grouped and sampled for chunk 356 of 356.\n"
     ]
    }
   ],
   "source": [
    "df_merged_temps = []\n",
    "\n",
    "for i in (range(len(merged_chunks))):\n",
    "    df_merged_temp = merged_chunks[i].groupby('index').sample(n=1).reset_index(drop=True)\n",
    "    df_merged_temps.append(df_merged_temp)\n",
    "    print(f'Grouped and sampled for chunk {i+1} of {total_chunks}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe28073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the merged chunks into a single DataFrame\n",
    "df_merged = pd.concat(df_merged_temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09f7eb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people: 17760502.\n",
      "________________\n",
      "Number of activities: 22946724.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of people: {len(df_merged)}.')\n",
    "print('________________')\n",
    "print(f\"Number of activities: {df_merged['activities'].sum()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "41b02131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-06-18 11:16:33\n",
      "End Time: 2023-06-18 11:16:50\n",
      "–––––––––––––––––––\n",
      "22946724\n",
      "–––––––––––––––––––\n",
      "   index    age gender  educ                                       geometry  \\\n",
      "0    628  36-45   male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
      "1    628  36-45   male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
      "2    628  36-45   male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
      "\n",
      "   age_det main_act profile_id  vehicle_owned  trips_count  activities  \\\n",
      "0       39     work        420              1            6           5   \n",
      "1       39     work        420              1            6           5   \n",
      "2       39     work        420              1            6           5   \n",
      "\n",
      "   repetition  \n",
      "0           1  \n",
      "1           2  \n",
      "2           3  \n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# Code to execute\n",
    "scaled_df = df_merged.iloc[np.repeat(np.arange(len(df_merged)), df_merged['activities'])].reset_index(drop=True)\n",
    "scaled_df['repetition'] = scaled_df.groupby('index').cumcount() + 1\n",
    "# Get the current time after the code execution\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "# Print the start and end time\n",
    "print(\"End Time:\", end_time)\n",
    "print('–––––––––––––––––––')\n",
    "print(len(scaled_df))\n",
    "print('–––––––––––––––––––')\n",
    "print(scaled_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f32ef18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-06-18 11:17:59\n",
      "End Time: 2023-06-18 11:18:16\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# Create the 'activity_id' column\n",
    "scaled_df['act_id'] = scaled_df['index'].astype(str) + '.' + scaled_df['repetition'].astype(str)\n",
    "\n",
    "# Get the current time after the code execution\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98229c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-06-18 11:20:02\n",
      "End Time: 2023-06-18 11:20:11\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# Reset the index of the scaled DataFrame\n",
    "scaled_df = scaled_df.reset_index(drop=True)\n",
    "\n",
    "# Get the current time after the code execution\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "11a8cff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>educ</th>\n",
       "      <th>geometry</th>\n",
       "      <th>age_det</th>\n",
       "      <th>main_act</th>\n",
       "      <th>profile_id</th>\n",
       "      <th>vehicle_owned</th>\n",
       "      <th>trips_count</th>\n",
       "      <th>activities</th>\n",
       "      <th>repetition</th>\n",
       "      <th>act_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.302296500000004 29.836736299999988)</td>\n",
       "      <td>39</td>\n",
       "      <td>work</td>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>628.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.302296500000004 29.836736299999988)</td>\n",
       "      <td>39</td>\n",
       "      <td>work</td>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>628.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>628</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.302296500000004 29.836736299999988)</td>\n",
       "      <td>39</td>\n",
       "      <td>work</td>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>628.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>628</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.302296500000004 29.836736299999988)</td>\n",
       "      <td>39</td>\n",
       "      <td>work</td>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>628.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>628</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.302296500000004 29.836736299999988)</td>\n",
       "      <td>39</td>\n",
       "      <td>work</td>\n",
       "      <td>420</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>628.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>826</td>\n",
       "      <td>&lt;18</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (31.310103099999996 29.8016195)</td>\n",
       "      <td>3</td>\n",
       "      <td>primary</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>826.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>826</td>\n",
       "      <td>&lt;18</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (31.310103099999996 29.8016195)</td>\n",
       "      <td>3</td>\n",
       "      <td>primary</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>826.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>850</td>\n",
       "      <td>&lt;18</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (31.3040036 29.83853570000001)</td>\n",
       "      <td>11</td>\n",
       "      <td>primary</td>\n",
       "      <td>1607</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>850.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1171</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.3553709 29.84492519999999)</td>\n",
       "      <td>27</td>\n",
       "      <td>work</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1171.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;18</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (31.349656703769913 29.840759526980175)</td>\n",
       "      <td>13</td>\n",
       "      <td>secondary</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2019.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    age  gender  educ                                       geometry  \\\n",
       "0    628  36-45    male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
       "1    628  36-45    male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
       "2    628  36-45    male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
       "3    628  36-45    male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
       "4    628  36-45    male   0.0  POINT (31.302296500000004 29.836736299999988)   \n",
       "5    826    <18    male   1.0          POINT (31.310103099999996 29.8016195)   \n",
       "6    826    <18    male   1.0          POINT (31.310103099999996 29.8016195)   \n",
       "7    850    <18  female   1.0           POINT (31.3040036 29.83853570000001)   \n",
       "8   1171  26-35  female   0.0           POINT (31.3553709 29.84492519999999)   \n",
       "9   2019    <18    male   1.0  POINT (31.349656703769913 29.840759526980175)   \n",
       "\n",
       "   age_det   main_act profile_id  vehicle_owned  trips_count  activities  \\\n",
       "0       39       work        420              1            6           5   \n",
       "1       39       work        420              1            6           5   \n",
       "2       39       work        420              1            6           5   \n",
       "3       39       work        420              1            6           5   \n",
       "4       39       work        420              1            6           5   \n",
       "5        3    primary       1606              0            3           2   \n",
       "6        3    primary       1606              0            3           2   \n",
       "7       11    primary       1607              0            2           1   \n",
       "8       27       work        202              0            2           1   \n",
       "9       13  secondary       1606              0            3           2   \n",
       "\n",
       "   repetition  act_id  \n",
       "0           1   628.1  \n",
       "1           2   628.2  \n",
       "2           3   628.3  \n",
       "3           4   628.4  \n",
       "4           5   628.5  \n",
       "5           1   826.1  \n",
       "6           2   826.2  \n",
       "7           1   850.1  \n",
       "8           1  1171.1  \n",
       "9           1  2019.1  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e226b528",
   "metadata": {},
   "source": [
    "Trip distribution from OD+TI\n",
    "work        56%\n",
    "personal    39%\n",
    "shopping     5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de30b6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-18 11:46:25] Processed 0 rows...\n",
      "[2023-06-18 11:46:25] Processed 100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 1900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 2900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 3900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 4900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 5900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 6900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 7900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 8900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 9900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 10900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 11900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 12900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 13900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 14900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 15900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 16900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 17900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 18900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 19900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 20900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 21900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22000000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22100000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22200000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22300000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22400000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22500000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22600000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22700000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22800000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22900000 rows...\n",
      "[2023-06-18 11:46:25] Processed 22946724 rows...\n"
     ]
    }
   ],
   "source": [
    "# Create a boolean mask for rows that require updating\n",
    "mask = scaled_df['activities'] < 3\n",
    "\n",
    "# Define choices and weights for rows with activities < 3\n",
    "choices_low_activities = ['main_occ', 'personal', 'shopping']\n",
    "weights_low_activities = [0.56, 0.39, 0.05]\n",
    "\n",
    "# Update 'activity' column for rows with activities < 3\n",
    "scaled_df.loc[mask, 'activity'] = np.random.choice(choices_low_activities, size=mask.sum(), p=weights_low_activities)\n",
    "\n",
    "# Create a boolean mask for rows that require further update\n",
    "mask = ~mask\n",
    "\n",
    "# Define choices and weights for rows with activities >= 3\n",
    "choices_high_activities = ['main_occ', 'personal', 'shopping', 'home']\n",
    "weights_high_activities = [0.48, 0.32, 0.05, 0.15]\n",
    "\n",
    "# Get the suffix values for rows with activities >= 3\n",
    "suffix_values = scaled_df.loc[mask, 'activities'].astype(str).str[-1]\n",
    "\n",
    "# Create a boolean mask for rows where suffix values match the conditions\n",
    "suffix_mask = (suffix_values == '1') | (suffix_values == scaled_df.loc[mask, 'activities'].astype(str))\n",
    "\n",
    "# Update 'activity' column for matching rows with activities >= 3\n",
    "scaled_df.loc[mask & suffix_mask, 'activity'] = np.random.choice(choices_high_activities, size=suffix_mask.sum(), p=weights_high_activities)\n",
    "\n",
    "# Update 'activity' column for non-matching rows with activities >= 3\n",
    "scaled_df.loc[mask & ~suffix_mask, 'activity'] = np.random.choice(choices_low_activities, size=(mask & ~suffix_mask).sum(), p=weights_low_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e7ae29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the current time\n",
    "# start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "# print(\"Start Time:\", start_time)\n",
    "# print(\"Number of rows:\", len(scaled_df))\n",
    "\n",
    "# for index, row in scaled_df.iterrows():\n",
    "#     if row['activities'] < 3:\n",
    "#         choices = ['main_occ', 'personal', 'shopping']\n",
    "#         weights = [0.56, 0.39, 0.05]\n",
    "#         scaled_df.at[index, 'activity'] = np.random.choice(choices, p=weights)\n",
    "#     else:\n",
    "#         ends_with = [1, row['activities']]\n",
    "#         if str(row['act_id']).endswith(tuple(map(str, ends_with))):\n",
    "#             choices = ['main_occ', 'personal', 'shopping']\n",
    "#             weights = [0.56, 0.39, 0.05]\n",
    "#             scaled_df.at[index, 'activity'] = np.random.choice(choices, p=weights)\n",
    "#         else:\n",
    "#             choices = ['main_occ', 'personal', 'shopping', 'home']\n",
    "#             weights = [0.48, 0.32, 0.05, 0.15]\n",
    "#             scaled_df.at[index, 'activity'] = np.random.choice(choices, p=weights)\n",
    "\n",
    "# # Get the current time after the code execution\n",
    "# end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "# print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "605397ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-06-18 11:50:20\n",
      "End Time: 2023-06-18 11:50:24\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# Create a boolean mask for rows where 'activity' is 'main_occ'\n",
    "mask = scaled_df['activity'] == 'main_occ'\n",
    "\n",
    "# Update the 'activity' column using vectorized operations\n",
    "scaled_df.loc[mask, 'activity'] = scaled_df.loc[mask, 'main_act']\n",
    "\n",
    "# Get the current time after the code execution\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "55003dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the current time\n",
    "# start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "# print(\"Start Time:\", start_time)\n",
    "\n",
    "# for index, row in scaled_df.iterrows():\n",
    "#     if row['activity'] == 'main_occ':\n",
    "#         scaled_df.at[index, 'activity'] = scaled_df.at[index, 'main_act']\n",
    "        \n",
    "# # Get the current time after the code execution\n",
    "# end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "# print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf27f1",
   "metadata": {},
   "source": [
    "### Attach trip distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "af0f44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "distmean = trips['distance'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "09bc2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only once \n",
    "for index, row in trips.iterrows():\n",
    "    if row['trip_purpose'] == 'edu' and row['age'] != '<18':\n",
    "        trips.at[index, 'trip_purpose'] = 'uni'\n",
    "    elif row['trip_purpose'] == 'edu' and row['distance'] > 6:\n",
    "        trips.at[index, 'trip_purpose'] = 'secondary'\n",
    "    elif row['trip_purpose'] == 'edu' and row['distance'] <= 6:\n",
    "        trips.at[index, 'trip_purpose'] = 'primary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bba8ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.rename(columns={'trip_purpose': 'activity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ed7a3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = trips[['age','gender','activity','distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "64ef9a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>activity</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>work</td>\n",
       "      <td>7.727921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>work</td>\n",
       "      <td>6.529161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-25</td>\n",
       "      <td>female</td>\n",
       "      <td>personal</td>\n",
       "      <td>9.302396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46-55</td>\n",
       "      <td>female</td>\n",
       "      <td>personal</td>\n",
       "      <td>2.162127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-25</td>\n",
       "      <td>female</td>\n",
       "      <td>home</td>\n",
       "      <td>9.691204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11880</th>\n",
       "      <td>36-45</td>\n",
       "      <td>female</td>\n",
       "      <td>shopping</td>\n",
       "      <td>0.714051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11881</th>\n",
       "      <td>56-65</td>\n",
       "      <td>male</td>\n",
       "      <td>home</td>\n",
       "      <td>3.909081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11882</th>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>work</td>\n",
       "      <td>10.849445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11883</th>\n",
       "      <td>18-25</td>\n",
       "      <td>male</td>\n",
       "      <td>home</td>\n",
       "      <td>82.570406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11884</th>\n",
       "      <td>46-55</td>\n",
       "      <td>male</td>\n",
       "      <td>personal</td>\n",
       "      <td>13.764800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11885 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  activity   distance\n",
       "0      26-35  female      work   7.727921\n",
       "1      36-45    male      work   6.529161\n",
       "2      18-25  female  personal   9.302396\n",
       "3      46-55  female  personal   2.162127\n",
       "4      18-25  female      home   9.691204\n",
       "...      ...     ...       ...        ...\n",
       "11880  36-45  female  shopping   0.714051\n",
       "11881  56-65    male      home   3.909081\n",
       "11882  36-45    male      work  10.849445\n",
       "11883  18-25    male      home  82.570406\n",
       "11884  46-55    male  personal  13.764800\n",
       "\n",
       "[11885 rows x 4 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e0bb647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 of 459\n",
      "Chunk 2 of 459\n",
      "Chunk 3 of 459\n",
      "Chunk 4 of 459\n",
      "Chunk 5 of 459\n",
      "Chunk 6 of 459\n",
      "Chunk 7 of 459\n",
      "Chunk 8 of 459\n",
      "Chunk 9 of 459\n",
      "Chunk 10 of 459\n",
      "Chunk 11 of 459\n",
      "Chunk 12 of 459\n",
      "Chunk 13 of 459\n",
      "Chunk 14 of 459\n",
      "Chunk 15 of 459\n",
      "Chunk 16 of 459\n",
      "Chunk 17 of 459\n",
      "Chunk 18 of 459\n",
      "Chunk 19 of 459\n",
      "Chunk 20 of 459\n",
      "Chunk 21 of 459\n",
      "Chunk 22 of 459\n",
      "Chunk 23 of 459\n",
      "Chunk 24 of 459\n",
      "Chunk 25 of 459\n",
      "Chunk 26 of 459\n",
      "Chunk 27 of 459\n",
      "Chunk 28 of 459\n",
      "Chunk 29 of 459\n",
      "Chunk 30 of 459\n",
      "Chunk 31 of 459\n",
      "Chunk 32 of 459\n",
      "Chunk 33 of 459\n",
      "Chunk 34 of 459\n",
      "Chunk 35 of 459\n",
      "Chunk 36 of 459\n",
      "Chunk 37 of 459\n",
      "Chunk 38 of 459\n",
      "Chunk 39 of 459\n",
      "Chunk 40 of 459\n",
      "Chunk 41 of 459\n",
      "Chunk 42 of 459\n",
      "Chunk 43 of 459\n",
      "Chunk 44 of 459\n",
      "Chunk 45 of 459\n",
      "Chunk 46 of 459\n",
      "Chunk 47 of 459\n",
      "Chunk 48 of 459\n",
      "Chunk 49 of 459\n",
      "Chunk 50 of 459\n",
      "Chunk 51 of 459\n",
      "Chunk 52 of 459\n",
      "Chunk 53 of 459\n",
      "Chunk 54 of 459\n",
      "Chunk 55 of 459\n",
      "Chunk 56 of 459\n",
      "Chunk 57 of 459\n",
      "Chunk 58 of 459\n",
      "Chunk 59 of 459\n",
      "Chunk 60 of 459\n",
      "Chunk 61 of 459\n",
      "Chunk 62 of 459\n",
      "Chunk 63 of 459\n",
      "Chunk 64 of 459\n",
      "Chunk 65 of 459\n",
      "Chunk 66 of 459\n",
      "Chunk 67 of 459\n",
      "Chunk 68 of 459\n",
      "Chunk 69 of 459\n",
      "Chunk 70 of 459\n",
      "Chunk 71 of 459\n",
      "Chunk 72 of 459\n",
      "Chunk 73 of 459\n",
      "Chunk 74 of 459\n",
      "Chunk 75 of 459\n",
      "Chunk 76 of 459\n",
      "Chunk 77 of 459\n",
      "Chunk 78 of 459\n",
      "Chunk 79 of 459\n",
      "Chunk 80 of 459\n",
      "Chunk 81 of 459\n",
      "Chunk 82 of 459\n",
      "Chunk 83 of 459\n",
      "Chunk 84 of 459\n",
      "Chunk 85 of 459\n",
      "Chunk 86 of 459\n",
      "Chunk 87 of 459\n",
      "Chunk 88 of 459\n",
      "Chunk 89 of 459\n",
      "Chunk 90 of 459\n",
      "Chunk 91 of 459\n",
      "Chunk 92 of 459\n",
      "Chunk 93 of 459\n",
      "Chunk 94 of 459\n",
      "Chunk 95 of 459\n",
      "Chunk 96 of 459\n",
      "Chunk 97 of 459\n",
      "Chunk 98 of 459\n",
      "Chunk 99 of 459\n",
      "Chunk 100 of 459\n",
      "Chunk 101 of 459\n",
      "Chunk 102 of 459\n",
      "Chunk 103 of 459\n",
      "Chunk 104 of 459\n",
      "Chunk 105 of 459\n",
      "Chunk 106 of 459\n",
      "Chunk 107 of 459\n",
      "Chunk 108 of 459\n",
      "Chunk 109 of 459\n",
      "Chunk 110 of 459\n",
      "Chunk 111 of 459\n",
      "Chunk 112 of 459\n",
      "Chunk 113 of 459\n",
      "Chunk 114 of 459\n",
      "Chunk 115 of 459\n",
      "Chunk 116 of 459\n",
      "Chunk 117 of 459\n",
      "Chunk 118 of 459\n",
      "Chunk 119 of 459\n",
      "Chunk 120 of 459\n",
      "Chunk 121 of 459\n",
      "Chunk 122 of 459\n",
      "Chunk 123 of 459\n",
      "Chunk 124 of 459\n",
      "Chunk 125 of 459\n",
      "Chunk 126 of 459\n",
      "Chunk 127 of 459\n",
      "Chunk 128 of 459\n",
      "Chunk 129 of 459\n",
      "Chunk 130 of 459\n",
      "Chunk 131 of 459\n",
      "Chunk 132 of 459\n",
      "Chunk 133 of 459\n",
      "Chunk 134 of 459\n",
      "Chunk 135 of 459\n",
      "Chunk 136 of 459\n",
      "Chunk 137 of 459\n",
      "Chunk 138 of 459\n",
      "Chunk 139 of 459\n",
      "Chunk 140 of 459\n",
      "Chunk 141 of 459\n",
      "Chunk 142 of 459\n",
      "Chunk 143 of 459\n",
      "Chunk 144 of 459\n",
      "Chunk 145 of 459\n",
      "Chunk 146 of 459\n",
      "Chunk 147 of 459\n",
      "Chunk 148 of 459\n",
      "Chunk 149 of 459\n",
      "Chunk 150 of 459\n",
      "Chunk 151 of 459\n",
      "Chunk 152 of 459\n",
      "Chunk 153 of 459\n",
      "Chunk 154 of 459\n",
      "Chunk 155 of 459\n",
      "Chunk 156 of 459\n",
      "Chunk 157 of 459\n",
      "Chunk 158 of 459\n",
      "Chunk 159 of 459\n",
      "Chunk 160 of 459\n",
      "Chunk 161 of 459\n",
      "Chunk 162 of 459\n",
      "Chunk 163 of 459\n",
      "Chunk 164 of 459\n",
      "Chunk 165 of 459\n",
      "Chunk 166 of 459\n",
      "Chunk 167 of 459\n",
      "Chunk 168 of 459\n",
      "Chunk 169 of 459\n",
      "Chunk 170 of 459\n",
      "Chunk 171 of 459\n",
      "Chunk 172 of 459\n",
      "Chunk 173 of 459\n",
      "Chunk 174 of 459\n",
      "Chunk 175 of 459\n",
      "Chunk 176 of 459\n",
      "Chunk 177 of 459\n",
      "Chunk 178 of 459\n",
      "Chunk 179 of 459\n",
      "Chunk 180 of 459\n",
      "Chunk 181 of 459\n",
      "Chunk 182 of 459\n",
      "Chunk 183 of 459\n",
      "Chunk 184 of 459\n",
      "Chunk 185 of 459\n",
      "Chunk 186 of 459\n",
      "Chunk 187 of 459\n",
      "Chunk 188 of 459\n",
      "Chunk 189 of 459\n",
      "Chunk 190 of 459\n",
      "Chunk 191 of 459\n",
      "Chunk 192 of 459\n",
      "Chunk 193 of 459\n",
      "Chunk 194 of 459\n",
      "Chunk 195 of 459\n",
      "Chunk 196 of 459\n",
      "Chunk 197 of 459\n",
      "Chunk 198 of 459\n",
      "Chunk 199 of 459\n",
      "Chunk 200 of 459\n",
      "Chunk 201 of 459\n",
      "Chunk 202 of 459\n",
      "Chunk 203 of 459\n",
      "Chunk 204 of 459\n",
      "Chunk 205 of 459\n",
      "Chunk 206 of 459\n",
      "Chunk 207 of 459\n",
      "Chunk 208 of 459\n",
      "Chunk 209 of 459\n",
      "Chunk 210 of 459\n",
      "Chunk 211 of 459\n",
      "Chunk 212 of 459\n",
      "Chunk 213 of 459\n",
      "Chunk 214 of 459\n",
      "Chunk 215 of 459\n",
      "Chunk 216 of 459\n",
      "Chunk 217 of 459\n",
      "Chunk 218 of 459\n",
      "Chunk 219 of 459\n",
      "Chunk 220 of 459\n",
      "Chunk 221 of 459\n",
      "Chunk 222 of 459\n",
      "Chunk 223 of 459\n",
      "Chunk 224 of 459\n",
      "Chunk 225 of 459\n",
      "Chunk 226 of 459\n",
      "Chunk 227 of 459\n",
      "Chunk 228 of 459\n",
      "Chunk 229 of 459\n",
      "Chunk 230 of 459\n",
      "Chunk 231 of 459\n",
      "Chunk 232 of 459\n",
      "Chunk 233 of 459\n",
      "Chunk 234 of 459\n",
      "Chunk 235 of 459\n",
      "Chunk 236 of 459\n",
      "Chunk 237 of 459\n",
      "Chunk 238 of 459\n",
      "Chunk 239 of 459\n",
      "Chunk 240 of 459\n",
      "Chunk 241 of 459\n",
      "Chunk 242 of 459\n",
      "Chunk 243 of 459\n",
      "Chunk 244 of 459\n",
      "Chunk 245 of 459\n",
      "Chunk 246 of 459\n",
      "Chunk 247 of 459\n",
      "Chunk 248 of 459\n",
      "Chunk 249 of 459\n",
      "Chunk 250 of 459\n",
      "Chunk 251 of 459\n",
      "Chunk 252 of 459\n",
      "Chunk 253 of 459\n",
      "Chunk 254 of 459\n",
      "Chunk 255 of 459\n",
      "Chunk 256 of 459\n",
      "Chunk 257 of 459\n",
      "Chunk 258 of 459\n",
      "Chunk 259 of 459\n",
      "Chunk 260 of 459\n",
      "Chunk 261 of 459\n",
      "Chunk 262 of 459\n",
      "Chunk 263 of 459\n",
      "Chunk 264 of 459\n",
      "Chunk 265 of 459\n",
      "Chunk 266 of 459\n",
      "Chunk 267 of 459\n",
      "Chunk 268 of 459\n",
      "Chunk 269 of 459\n",
      "Chunk 270 of 459\n",
      "Chunk 271 of 459\n",
      "Chunk 272 of 459\n",
      "Chunk 273 of 459\n",
      "Chunk 274 of 459\n",
      "Chunk 275 of 459\n",
      "Chunk 276 of 459\n",
      "Chunk 277 of 459\n",
      "Chunk 278 of 459\n",
      "Chunk 279 of 459\n",
      "Chunk 280 of 459\n",
      "Chunk 281 of 459\n",
      "Chunk 282 of 459\n",
      "Chunk 283 of 459\n",
      "Chunk 284 of 459\n",
      "Chunk 285 of 459\n",
      "Chunk 286 of 459\n",
      "Chunk 287 of 459\n",
      "Chunk 288 of 459\n",
      "Chunk 289 of 459\n",
      "Chunk 290 of 459\n",
      "Chunk 291 of 459\n",
      "Chunk 292 of 459\n",
      "Chunk 293 of 459\n",
      "Chunk 294 of 459\n",
      "Chunk 295 of 459\n",
      "Chunk 296 of 459\n",
      "Chunk 297 of 459\n",
      "Chunk 298 of 459\n",
      "Chunk 299 of 459\n",
      "Chunk 300 of 459\n",
      "Chunk 301 of 459\n",
      "Chunk 302 of 459\n",
      "Chunk 303 of 459\n",
      "Chunk 304 of 459\n",
      "Chunk 305 of 459\n",
      "Chunk 306 of 459\n",
      "Chunk 307 of 459\n",
      "Chunk 308 of 459\n",
      "Chunk 309 of 459\n",
      "Chunk 310 of 459\n",
      "Chunk 311 of 459\n",
      "Chunk 312 of 459\n",
      "Chunk 313 of 459\n",
      "Chunk 314 of 459\n",
      "Chunk 315 of 459\n",
      "Chunk 316 of 459\n",
      "Chunk 317 of 459\n",
      "Chunk 318 of 459\n",
      "Chunk 319 of 459\n",
      "Chunk 320 of 459\n",
      "Chunk 321 of 459\n",
      "Chunk 322 of 459\n",
      "Chunk 323 of 459\n",
      "Chunk 324 of 459\n",
      "Chunk 325 of 459\n",
      "Chunk 326 of 459\n",
      "Chunk 327 of 459\n",
      "Chunk 328 of 459\n",
      "Chunk 329 of 459\n",
      "Chunk 330 of 459\n",
      "Chunk 331 of 459\n",
      "Chunk 332 of 459\n",
      "Chunk 333 of 459\n",
      "Chunk 334 of 459\n",
      "Chunk 335 of 459\n",
      "Chunk 336 of 459\n",
      "Chunk 337 of 459\n",
      "Chunk 338 of 459\n",
      "Chunk 339 of 459\n",
      "Chunk 340 of 459\n",
      "Chunk 341 of 459\n",
      "Chunk 342 of 459\n",
      "Chunk 343 of 459\n",
      "Chunk 344 of 459\n",
      "Chunk 345 of 459\n",
      "Chunk 346 of 459\n",
      "Chunk 347 of 459\n",
      "Chunk 348 of 459\n",
      "Chunk 349 of 459\n",
      "Chunk 350 of 459\n",
      "Chunk 351 of 459\n",
      "Chunk 352 of 459\n",
      "Chunk 353 of 459\n",
      "Chunk 354 of 459\n",
      "Chunk 355 of 459\n",
      "Chunk 356 of 459\n",
      "Chunk 357 of 459\n",
      "Chunk 358 of 459\n",
      "Chunk 359 of 459\n",
      "Chunk 360 of 459\n",
      "Chunk 361 of 459\n",
      "Chunk 362 of 459\n",
      "Chunk 363 of 459\n",
      "Chunk 364 of 459\n",
      "Chunk 365 of 459\n",
      "Chunk 366 of 459\n",
      "Chunk 367 of 459\n",
      "Chunk 368 of 459\n",
      "Chunk 369 of 459\n",
      "Chunk 370 of 459\n",
      "Chunk 371 of 459\n",
      "Chunk 372 of 459\n",
      "Chunk 373 of 459\n",
      "Chunk 374 of 459\n",
      "Chunk 375 of 459\n",
      "Chunk 376 of 459\n",
      "Chunk 377 of 459\n",
      "Chunk 378 of 459\n",
      "Chunk 379 of 459\n",
      "Chunk 380 of 459\n",
      "Chunk 381 of 459\n",
      "Chunk 382 of 459\n",
      "Chunk 383 of 459\n",
      "Chunk 384 of 459\n",
      "Chunk 385 of 459\n",
      "Chunk 386 of 459\n",
      "Chunk 387 of 459\n",
      "Chunk 388 of 459\n",
      "Chunk 389 of 459\n",
      "Chunk 390 of 459\n",
      "Chunk 391 of 459\n",
      "Chunk 392 of 459\n",
      "Chunk 393 of 459\n",
      "Chunk 394 of 459\n",
      "Chunk 395 of 459\n",
      "Chunk 396 of 459\n",
      "Chunk 397 of 459\n",
      "Chunk 398 of 459\n",
      "Chunk 399 of 459\n",
      "Chunk 400 of 459\n",
      "Chunk 401 of 459\n",
      "Chunk 402 of 459\n",
      "Chunk 403 of 459\n",
      "Chunk 404 of 459\n",
      "Chunk 405 of 459\n",
      "Chunk 406 of 459\n",
      "Chunk 407 of 459\n",
      "Chunk 408 of 459\n",
      "Chunk 409 of 459\n",
      "Chunk 410 of 459\n",
      "Chunk 411 of 459\n",
      "Chunk 412 of 459\n",
      "Chunk 413 of 459\n",
      "Chunk 414 of 459\n",
      "Chunk 415 of 459\n",
      "Chunk 416 of 459\n",
      "Chunk 417 of 459\n",
      "Chunk 418 of 459\n",
      "Chunk 419 of 459\n",
      "Chunk 420 of 459\n",
      "Chunk 421 of 459\n",
      "Chunk 422 of 459\n",
      "Chunk 423 of 459\n",
      "Chunk 424 of 459\n",
      "Chunk 425 of 459\n",
      "Chunk 426 of 459\n",
      "Chunk 427 of 459\n",
      "Chunk 428 of 459\n",
      "Chunk 429 of 459\n",
      "Chunk 430 of 459\n",
      "Chunk 431 of 459\n",
      "Chunk 432 of 459\n",
      "Chunk 433 of 459\n",
      "Chunk 434 of 459\n",
      "Chunk 435 of 459\n",
      "Chunk 436 of 459\n",
      "Chunk 437 of 459\n",
      "Chunk 438 of 459\n",
      "Chunk 439 of 459\n",
      "Chunk 440 of 459\n",
      "Chunk 441 of 459\n",
      "Chunk 442 of 459\n",
      "Chunk 443 of 459\n",
      "Chunk 444 of 459\n",
      "Chunk 445 of 459\n",
      "Chunk 446 of 459\n",
      "Chunk 447 of 459\n",
      "Chunk 448 of 459\n",
      "Chunk 449 of 459\n",
      "Chunk 450 of 459\n",
      "Chunk 451 of 459\n",
      "Chunk 452 of 459\n",
      "Chunk 453 of 459\n",
      "Chunk 454 of 459\n",
      "Chunk 455 of 459\n",
      "Chunk 456 of 459\n",
      "Chunk 457 of 459\n",
      "Chunk 458 of 459\n",
      "Chunk 459 of 459\n"
     ]
    }
   ],
   "source": [
    "# Define the chunk size\n",
    "chunksize = 50000\n",
    "\n",
    "# Create an empty list to store the merged chunks\n",
    "merged_chunks = []\n",
    "\n",
    "# Get the total number of chunks\n",
    "total_chunks = -(-len(scaled_df) // chunksize)  # Round up division\n",
    "\n",
    "# Iterate over the data in chunks\n",
    "for i in range(total_chunks):\n",
    "    start = i * chunksize\n",
    "    end = (i + 1) * chunksize\n",
    "    # Get the chunk of data\n",
    "    chunk_df = scaled_df[start:end]\n",
    "    # Perform the merge operation on each chunk\n",
    "    merged_chunk = pd.merge(chunk_df, trips, on=['age', 'gender','activity'], how='left')    \n",
    "    # Delete all but one\n",
    "    merged_chunk = merged_chunk.groupby('act_id').sample(n=1).reset_index(drop=True)\n",
    "    # Append the merged chunk to the list\n",
    "    merged_chunks.append(merged_chunk)\n",
    "    print(f'Chunk {i+1} of {total_chunks}')\n",
    "    \n",
    "# Concatenate the merged chunks into a single DataFrame\n",
    "df_trips = pd.concat(merged_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "238d583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips['distance'].fillna(distmean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7586de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safety stop \n",
    "df_trips.to_csv('data/interim/activitychains/population_without_times.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47432e9",
   "metadata": {},
   "source": [
    "### Attach times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "474e33e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>educ</th>\n",
       "      <th>geometry</th>\n",
       "      <th>age_det</th>\n",
       "      <th>main_act</th>\n",
       "      <th>profile_id</th>\n",
       "      <th>vehicle_owned</th>\n",
       "      <th>trips_count</th>\n",
       "      <th>activities</th>\n",
       "      <th>repetition</th>\n",
       "      <th>act_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000133</td>\n",
       "      <td>&lt;18</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POINT (31.2400075 29.7918666)</td>\n",
       "      <td>12</td>\n",
       "      <td>secondary</td>\n",
       "      <td>1607</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000133.1</td>\n",
       "      <td>shopping</td>\n",
       "      <td>2.181599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10001363</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.27183177046704 30.11594197673126)</td>\n",
       "      <td>41</td>\n",
       "      <td>work</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10001363.1</td>\n",
       "      <td>personal</td>\n",
       "      <td>7.807914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10001363</td>\n",
       "      <td>36-45</td>\n",
       "      <td>male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POINT (31.27183177046704 30.11594197673126)</td>\n",
       "      <td>41</td>\n",
       "      <td>work</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10001363.2</td>\n",
       "      <td>work</td>\n",
       "      <td>8.054701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     index    age  gender  educ  \\\n",
       "0           0   1000133    <18  female   1.0   \n",
       "1           1  10001363  36-45    male   0.0   \n",
       "2           2  10001363  36-45    male   0.0   \n",
       "\n",
       "                                      geometry  age_det   main_act  \\\n",
       "0                POINT (31.2400075 29.7918666)       12  secondary   \n",
       "1  POINT (31.27183177046704 30.11594197673126)       41       work   \n",
       "2  POINT (31.27183177046704 30.11594197673126)       41       work   \n",
       "\n",
       "   profile_id  vehicle_owned  trips_count  activities  repetition      act_id  \\\n",
       "0        1607              0            2           1           1   1000133.1   \n",
       "1         101              0            3           2           1  10001363.1   \n",
       "2         101              0            3           2           2  10001363.2   \n",
       "\n",
       "   activity  distance  \n",
       "0  shopping  2.181599  \n",
       "1  personal  7.807914  \n",
       "2      work  8.054701  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aab95420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-06-18 21:59:24\n",
      "End Time: 2023-06-18 21:59:24\n"
     ]
    }
   ],
   "source": [
    "def select_trip(row):\n",
    "    if row['trips_count'] == 1:\n",
    "        selection = one_trip_chains.sample(n=1)\n",
    "        row['start'] = selection['start_1'].values[0]\n",
    "        row['end'] = selection['end_1'].values[0]   \n",
    "    elif row['trips_count'] == 2:\n",
    "        selection = two_trip_chains.sample(n=1)\n",
    "        row['start'] = selection['start_1'].values[0]\n",
    "        row['end'] = selection['end_1'].values[0]\n",
    "    elif row['trips_count'] == 3:\n",
    "        selection = three_trip_chains.sample(n=1)\n",
    "        if row['repetition'] == 1:\n",
    "            row['start'] = selection['start_0'].values[0]\n",
    "            row['end'] = selection['end_0'].values[0]\n",
    "        elif row['repetition'] == 2:\n",
    "            row['start'] = selection['start_1'].values[0]\n",
    "            row['end'] = selection['end_1'].values[0]\n",
    "    elif row['trips_count'] == 4:\n",
    "        selection = four_trip_chains.sample(n=1)\n",
    "        if row['repetition'] == 1:\n",
    "            row['start'] = selection['start_0'].values[0]\n",
    "            row['end'] = selection['end_0'].values[0]\n",
    "        elif row['repetition'] == 2:\n",
    "            row['start'] = selection['start_1'].values[0]\n",
    "            row['end'] = selection['end_1'].values[0]\n",
    "        elif row['repetition'] == 3:\n",
    "            row['start'] = selection['start_2'].values[0]\n",
    "            row['end'] = selection['end_2'].values[0]\n",
    "    elif row['trips_count'] == 5:\n",
    "        selection = five_trip_chains.sample(n=1)\n",
    "        if row['repetition'] == 1:\n",
    "            row['start'] = selection['start_0'].values[0]\n",
    "            row['end'] = selection['end_0'].values[0]\n",
    "        elif row['repetition'] == 2:\n",
    "            row['start'] = selection['start_1'].values[0]\n",
    "            row['end'] = selection['end_1'].values[0]\n",
    "        elif row['repetition'] == 3:\n",
    "            row['start'] = selection['start_2'].values[0]\n",
    "            row['end'] = selection['end_2'].values[0]\n",
    "        elif row['repetition'] == 4:\n",
    "            row['start'] = selection['start_3'].values[0]\n",
    "            row['end'] = selection['end_3'].values[0]\n",
    "    elif row['trips_count'] == 6:\n",
    "        selection = six_trip_chains.sample(n=1)\n",
    "        if row['repetition'] == 1:\n",
    "            row['start'] = selection['start_0'].values[0]\n",
    "            row['end'] = selection['end_0'].values[0]\n",
    "        elif row['repetition'] == 2:\n",
    "            row['start'] = selection['start_1'].values[0]\n",
    "            row['end'] = selection['end_1'].values[0]\n",
    "        elif row['repetition'] == 3:\n",
    "            row['start'] = selection['start_2'].values[0]\n",
    "            row['end'] = selection['end_2'].values[0]\n",
    "        elif row['repetition'] == 4:\n",
    "            row['start'] = selection['start_3'].values[0]\n",
    "            row['end'] = selection['end_3'].values[0]\n",
    "        elif row['repetition'] == 5:\n",
    "            row['start'] = selection['start_4'].values[0]\n",
    "            row['end'] = selection['end_4'].values[0]       \n",
    "    return row\n",
    "\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "# Apply formula\n",
    "activitychains = df_trips.apply(select_trip, axis=1)\n",
    "\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e99a12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Time: 2023-06-19 05:51:40\n"
     ]
    }
   ],
   "source": [
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930489be",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "010e0ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22946724 entries, 0 to 46723\n",
      "Data columns (total 17 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   index          int64  \n",
      " 1   age            object \n",
      " 2   gender         object \n",
      " 3   educ           float64\n",
      " 4   geometry       object \n",
      " 5   age_det        int64  \n",
      " 6   main_act       object \n",
      " 7   profile_id     object \n",
      " 8   vehicle_owned  int64  \n",
      " 9   trips_count    int64  \n",
      " 10  activities     int64  \n",
      " 11  repetition     int64  \n",
      " 12  act_id         object \n",
      " 13  activity       object \n",
      " 14  distance       float64\n",
      " 15  start          float64\n",
      " 16  end            float64\n",
      "dtypes: float64(4), int64(6), object(7)\n",
      "memory usage: 3.1+ GB\n"
     ]
    }
   ],
   "source": [
    "activitychains.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ec88dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "activitychains.to_csv('data/interim/activitychains/population.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b7da8",
   "metadata": {},
   "source": [
    "## Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d815cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/interim/activitychains/population.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "61a34552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'profile_id', 'age_det', 'main_act', 'trips_count','educ'], axis=1)\n",
    "df.rename(columns={'geometry': 'home_loc'}, inplace=True)\n",
    "df.rename(columns={'vehicle_owned': 'car'}, inplace=True)\n",
    "df.rename(columns={'repetition': 'act_no'}, inplace=True)\n",
    "df.rename(columns={'index': 'person_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9227ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-07-07 17:54:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/smcvnv_s04v84f_dnqvd4xkw0000gn/T/ipykernel_44237/1450610785.py:34: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(duplicated_rows, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Time: 2023-07-07 17:55:39\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# Create a mask to identify rows with repetition value equal to 1\n",
    "mask = df['act_no'] == 1\n",
    "\n",
    "# Get the index positions of the rows that satisfy the condition\n",
    "indices = df.index[mask]\n",
    "\n",
    "# Duplicate rows that satisfy the condition and append them to the DataFrame\n",
    "duplicated_rows = df.loc[mask].copy()\n",
    "\n",
    "# Update the values of start and end columns in the duplicated rows\n",
    "duplicated_rows['start'] = np.nan\n",
    "duplicated_rows['end'] = 'tbda'\n",
    "\n",
    "# Convert the act_id column to string data type\n",
    "duplicated_rows['act_id'] = duplicated_rows['act_id'].astype(str)\n",
    "\n",
    "# Update the last letter of the act_id column in the duplicated rows\n",
    "duplicated_rows['act_id'] = duplicated_rows['act_id'].str[:-1] + '0'\n",
    "\n",
    "# Update the activity column in the duplicated rows\n",
    "duplicated_rows['activity'] = 'home'\n",
    "\n",
    "# Update the distance column in the duplicated rows\n",
    "duplicated_rows['distance'] = 0\n",
    "\n",
    "# Update the activity number column in the duplicated rows\n",
    "duplicated_rows['act_no'] = 0\n",
    "\n",
    "\n",
    "df = df.append(duplicated_rows, ignore_index=True)\n",
    "df['act_id'] = df['act_id'].astype(float)\n",
    "df = df.sort_values(by='act_id', ascending=True)\n",
    "\n",
    "# Get the current time\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c5fdcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-07-07 17:55:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/smcvnv_s04v84f_dnqvd4xkw0000gn/T/ipykernel_44237/3811636283.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(duplicated_rows, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Time: 2023-07-07 17:56:32\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# Create a mask to identify rows with repetition value equal to 1\n",
    "mask = df['activities'] == df['act_no']\n",
    "\n",
    "# Get the index positions of the rows that satisfy the condition\n",
    "indices = df.index[mask]\n",
    "\n",
    "# Duplicate rows that satisfy the condition and append them to the DataFrame\n",
    "duplicated_rows = df.loc[mask].copy()\n",
    "\n",
    "# Update the values of start and end columns in the duplicated rows\n",
    "duplicated_rows['start'] = 'tbdb'\n",
    "duplicated_rows['end'] = np.nan\n",
    "\n",
    "# Convert the act_id column to string data type\n",
    "duplicated_rows['act_id'] = duplicated_rows['act_id'].astype(str)\n",
    "\n",
    "# Update the activity column in the duplicated rows\n",
    "duplicated_rows['activity'] = 'home'\n",
    "\n",
    "# Update the distance column in the duplicated rows\n",
    "duplicated_rows['distance'] = 0\n",
    "\n",
    "# Update the act_no column in the duplicated rows\n",
    "duplicated_rows['act_no'] = duplicated_rows['activities'] + 1\n",
    "\n",
    "# Update the last letter of the act_id column in the duplicated rows\n",
    "duplicated_rows['act_id'] = duplicated_rows['act_id'].str[:-1] + (duplicated_rows['activities'] + 1).astype(str)\n",
    "\n",
    "df = df.append(duplicated_rows, ignore_index=True)\n",
    "df['activities'] = df['activities'] + 2\n",
    "df['act_id'] = df['act_id'].astype(float)\n",
    "df = df.sort_values(by='act_id', ascending=True)\n",
    "\n",
    "\n",
    "# Get the current time\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3bf299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a827dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming 25km/h average speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b0ad03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average distance\n",
    "average_dist = df[df['activity'] != 'home']['distance'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "26ac1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace distances of under 1km with average distance\n",
    "df.loc[(df['activity'] != 'home') & (df['distance'] < 1), 'distance'] = average_dist\n",
    "\n",
    "# replace distances of over 100km with average distance\n",
    "df.loc[df['distance'] > 100, 'distance'] = average_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a180f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-07-07 17:56:58\n",
      "End Time: 2023-07-07 17:57:17\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "home_dist_dict = []\n",
    "\n",
    "# Create a dictionary mapping 'person_id' to the corresponding 'distance' value\n",
    "home_dist_dict = df.loc[df['act_no'] == 1, ['person_id', 'distance']].set_index('person_id')['distance'].to_dict()\n",
    "\n",
    "# Create the 'home_dist' column by mapping the values from the dictionary\n",
    "df['home_dist'] = df['person_id'].map(home_dist_dict)\n",
    "\n",
    "# Get the current time\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cb40a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "451b67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = safety_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c2fa8aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "13907fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-07-07 18:04:09\n",
      "End Time: 2023-07-07 18:23:03\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "df_grouped = []\n",
    "\n",
    "# Create the grouped DataFrame with maximum 'home_dist' and 'start' for each 'person_id' where 'act_no' is 1\n",
    "df_grouped = df[df['act_no'] == 1].groupby('person_id').agg({'home_dist': 'max', 'start': 'max'}).rename(columns={'home_dist': 'home_dist_max', 'start': 'start_time'})\n",
    "\n",
    "# # Create a new DataFrame 'df_grouped' with maximum 'home_dist' and 'start_time' for each 'person_id'\n",
    "# df_grouped = df.loc[df['act_no'] == 1].groupby('person_id').max()['start']\n",
    "# df_grouped = df_grouped.to_frame()\n",
    "# df_grouped = df_grouped.rename(columns={'start': 'start_time'})\n",
    "\n",
    "# Get the current time\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c1068d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'df_test' DataFrame with 'df_grouped' on 'person_id' to update 'home_dist' and 'start_time'\n",
    "df = pd.merge(df, df_grouped, on='person_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0aa7f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2023-07-07 18:23:49\n",
      "End Time: 2023-07-07 18:24:08\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# assign first distance as home distance for intermediate or final home activity\n",
    "df.loc[(df['act_no'] != 0) & (df['act_no'] != df['activities']) & (df['activity'] == 'home'), 'distance'] = df['home_dist']\n",
    "\n",
    "# calculate end time of first home activity \n",
    "df.loc[df['act_no'] == 0, 'end'] = round(df['start_time'] - df['home_dist'] / 25 * 60)\n",
    "\n",
    "# clean df\n",
    "df.drop(columns=['home_dist','start_time'], inplace=True)\n",
    "\n",
    "# Get the current time\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "66d6cb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/smcvnv_s04v84f_dnqvd4xkw0000gn/T/ipykernel_44237/2549207887.py:4: FutureWarning: Dropping invalid columns in DataFrameGroupBy.max is deprecated. In a future version, a TypeError will be raised. Before calling .max, select only columns which should be valid for the function.\n",
      "  grouped_df = df.groupby('person_id').max()['last_act_end']\n"
     ]
    }
   ],
   "source": [
    "# Get the current time\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"Start Time:\", start_time)\n",
    "\n",
    "# set last activity as last_end_time\n",
    "df.loc[df['activity'] != 'home', 'last_act_end'] = df['end']\n",
    "\n",
    "grouped_df = df.groupby('person_id').max()['last_act_end']\n",
    "\n",
    "# Get the current time\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "print(\"End Time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4d81bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, grouped_df, on='person_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f16f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate start time of last home activity\n",
    "df.loc[\n",
    "    df['act_no'] == (df.loc[:, 'activities'] - 1),\n",
    "    'start'\n",
    "] = round(df.loc[:, 'last_act_end_y'] + df.loc[:, 'distance'] / 25 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d5a82e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean df\n",
    "df.drop(columns=['last_act_end_x', 'last_act_end_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "319bfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df[\"start\"].isna() & df[\"act_no\"] != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec282d",
   "metadata": {},
   "source": [
    "### Checking outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b8ebdfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips starting before the day: 0\n",
      "Number of trips ending before the day starts: 0\n",
      "Number of trips ending after midnight: 0\n",
      "––––––––––––\n",
      "Number of NAs at wrong location: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of trips starting before the day: {len(df[df[\"start\"] < 0])}')\n",
    "print(f'Number of trips ending before the day starts: {len(df[df[\"end\"] < 30])}')\n",
    "print(f'Number of trips ending after midnight: {len(df[df[\"end\"] > 1440])}')\n",
    "print('––––––––––––')\n",
    "print(f'Number of NAs at wrong location: {len(df[(df[\"start\"].isna() & df[\"act_no\"] != 0)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "565ce5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting 11 values that end before 00:30\n",
    "df = df[~(df[\"end\"] < 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7e23ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/interim/activitychains/population+home-act.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df7f08",
   "metadata": {},
   "source": [
    "## Working 27 june"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3bb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/interim/activitychains/population+home-act.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b490ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e822502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals: 15288918\n",
      "Number of trips: 53524560\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of individuals: {df['person_id'].nunique()}\")\n",
    "print(f'Number of trips: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41bdc565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>home_loc</th>\n",
       "      <th>car</th>\n",
       "      <th>activities</th>\n",
       "      <th>act_no</th>\n",
       "      <th>act_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>distance</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&gt;75</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.331430700000002 29.845431899999987)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>home</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>&gt;75</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.331430700000002 29.845431899999987)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>personal</td>\n",
       "      <td>5.902421</td>\n",
       "      <td>589.0</td>\n",
       "      <td>1132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>&gt;75</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.331430700000002 29.845431899999987)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>home</td>\n",
       "      <td>5.902421</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.313874 29.8144589)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>home</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.313874 29.8144589)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>work</td>\n",
       "      <td>14.937200</td>\n",
       "      <td>571.0</td>\n",
       "      <td>731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.313874 29.8144589)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>home</td>\n",
       "      <td>14.937200</td>\n",
       "      <td>767.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.3075189 29.860689800000003)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>home</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.3075189 29.860689800000003)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>personal</td>\n",
       "      <td>8.715996</td>\n",
       "      <td>897.0</td>\n",
       "      <td>1179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>26-35</td>\n",
       "      <td>female</td>\n",
       "      <td>POINT (31.3075189 29.860689800000003)</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>home</td>\n",
       "      <td>8.715996</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>26-35</td>\n",
       "      <td>male</td>\n",
       "      <td>POINT (31.345518647215872 29.847849488673337)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>home</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id    age  gender                                       home_loc  \\\n",
       "0          0    >75  female  POINT (31.331430700000002 29.845431899999987)   \n",
       "1          0    >75  female  POINT (31.331430700000002 29.845431899999987)   \n",
       "2          0    >75  female  POINT (31.331430700000002 29.845431899999987)   \n",
       "3          1  26-35  female                   POINT (31.313874 29.8144589)   \n",
       "4          1  26-35  female                   POINT (31.313874 29.8144589)   \n",
       "5          1  26-35  female                   POINT (31.313874 29.8144589)   \n",
       "6          2  26-35  female          POINT (31.3075189 29.860689800000003)   \n",
       "7          2  26-35  female          POINT (31.3075189 29.860689800000003)   \n",
       "8          2  26-35  female          POINT (31.3075189 29.860689800000003)   \n",
       "9          3  26-35    male  POINT (31.345518647215872 29.847849488673337)   \n",
       "\n",
       "   car  activities  act_no  act_id  activity   distance   start     end  \n",
       "0    1           3       0     0.0      home   0.000000    -inf   575.0  \n",
       "1    1           3       1     0.1  personal   5.902421   589.0  1132.0  \n",
       "2    1           3       2     0.2      home   5.902421  1146.0     inf  \n",
       "3    1           3       0     1.0      home   0.000000    -inf   535.0  \n",
       "4    1           3       1     1.1      work  14.937200   571.0   731.0  \n",
       "5    1           3       2     1.2      home  14.937200   767.0     inf  \n",
       "6    1           3       0     2.0      home   0.000000    -inf   876.0  \n",
       "7    1           3       1     2.1  personal   8.715996   897.0  1179.0  \n",
       "8    1           3       2     2.2      home   8.715996  1200.0     inf  \n",
       "9    0           3       0     3.0      home   0.000000    -inf   498.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c5929c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['home', 'personal', 'work', 'primary', 'shopping', 'uni',\n",
       "       'secondary'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['activity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4eb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
